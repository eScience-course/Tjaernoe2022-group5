{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bc80d9-c167-426c-950b-d815e92c864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "xr.set_options(display_style='html')\n",
    "import intake\n",
    "import cftime\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635909f3-5484-4d45-8841-b4a91fa43aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import from pangeo ###\n",
    "def read_pangeo(start_year, end_year, experimentid):\n",
    "    cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "    col = intake.open_esm_datastore(cat_url)\n",
    "    cat = col.search(source_id=['NorESM2-LM'], experiment_id=[experimentid], \n",
    "                     table_id=['day'], variable_id=['pr','clt','hus','va','tas'], member_id=['r1i1p1f1'])\n",
    "    dset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})\n",
    "    dataset_list = list(dset_dict.keys())\n",
    "    start_year_ = start_year\n",
    "    end_year_ = end_year\n",
    "    df = dset_dict[dataset_list[0]]\n",
    "    df_sliced = df.sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "    return df_sliced\n",
    "\n",
    "def read_to_detect(start_year, end_year, experimentid):\n",
    "    cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "    col = intake.open_esm_datastore(cat_url)\n",
    "    cat = col.search(source_id=['NorESM2-LM'], experiment_id=[experimentid], \n",
    "                     table_id=['day'], variable_id=['hus','va'], member_id=['r1i1p1f1'])\n",
    "    dset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})\n",
    "    dataset_list = list(dset_dict.keys())\n",
    "    start_year_ = start_year\n",
    "    end_year_ = end_year\n",
    "    df = dset_dict[dataset_list[0]]\n",
    "    df = df.chunk(20)\n",
    "    df_sliced = df.sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "    return df_sliced\n",
    "\n",
    "### AOD bias correction function ###\n",
    "\n",
    "\n",
    "### import from \"the bucket\" ###\n",
    "    \n",
    "def read_aod(start_year, end_year, experimentid):\n",
    "    s3 = s3fs.S3FileSystem(key=\"K1CQ7M1DMTLUFK182APD\", \n",
    "                           secret=\"3JuZAQm5I03jtpijCpHOdkAsJDNLNfZxBpM15Pi0\", \n",
    "                           client_kwargs=dict(endpoint_url=\"https://rgw.met.no\"))\n",
    "\n",
    "    if experimentid == 'historical': \n",
    "        s3path = list([\n",
    "        'escience2022/Ada/monthly/od550aer_AERday_NorESM2-LM_historical_r1i1p1f1_gn_2000101-20141231.nc'\n",
    "        ])\n",
    "        sopenlist=[s3.open(ss) for ss in s3path]\n",
    "        aod = (xr.open_mfdataset(sopenlist)).sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "\n",
    "    elif experimentid == 'ssp245':\n",
    "        s3path = list([\n",
    "            'escience2022/Ada/monthly/od550aer_AERday_NorESM2-LM_ssp245_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "        ])\n",
    "        sopenlist=[s3.open(ss) for ss in s3path]\n",
    "        aod = (xr.open_mfdataset(sopenlist)).sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "    \n",
    "    elif experimentid == 'ssp370':\n",
    "        s3path = list([\n",
    "            'escience2022/Ada/monthly/od550aer_AERday_NorESM2-LM_ssp370_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "        ])\n",
    "        sopenlist=[s3.open(ss) for ss in s3path]\n",
    "        aod = (xr.open_mfdataset(sopenlist)).sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "            \n",
    "    elif experimentid == 'ssp585':\n",
    "        s3path = list([\n",
    "             'escience2022/Ada/monthly/od550aer_AERday_NorESM2-LM_ssp585_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "        ])\n",
    "\n",
    "        sopenlist=[s3.open(ss) for ss in s3path]\n",
    "        aod = (xr.open_mfdataset(sopenlist)).sel(time = slice(str(start_year)+\"-01-01\", str(end_year)+\"-12-31\"))\n",
    "        \n",
    "    else:\n",
    "        print('something went wrong')\n",
    "\n",
    "    return aod\n",
    "    \n",
    "    \n",
    "def read_370(start_year, end_year):\n",
    "    s3 = s3fs.S3FileSystem(key=\"K1CQ7M1DMTLUFK182APD\", \n",
    "                           secret=\"3JuZAQm5I03jtpijCpHOdkAsJDNLNfZxBpM15Pi0\", \n",
    "                           client_kwargs=dict(endpoint_url=\"https://rgw.met.no\"))\n",
    "    s3path = list([\n",
    "        'escience2022/Remy/va_day_NorESM2-LM_ssp370_r1i1p1f1_gn_20810101-20901231.nc',\n",
    "        'escience2022/Remy/va_day_NorESM2-LM_ssp370_r1i1p1f1_gn_20910101-21001231.nc',\n",
    "        'escience2022/Remy/hus_day_NorESM2-LM_ssp370_r1i1p1f1_gn_20810101-20901231.nc',\n",
    "        'escience2022/Remy/hus_day_NorESM2-LM_ssp370_r1i1p1f1_gn_20910101-21001231.nc',\n",
    "    ])\n",
    "\n",
    "    sopenlist=[s3.open(ss) for ss in s3path]\n",
    "    bucket370 = xr.open_mfdataset(sopenlist).drop(('lat_bnds', 'time_bnds', 'lon_bnds')).sel(time = \n",
    "                                                                                             slice(str(start_year)+\"-01-01\",\n",
    "                                                                                                   str(end_year)+\"-12-31\"))\n",
    "    \n",
    "    return bucket370\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e40c0-3c21-4e83-b03c-f5b5548f7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data treatment'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a5502d-ca33-4843-9d38-f97abb921707",
   "metadata": {},
   "outputs": [],
   "source": [
    "aodh = read_aod(2000, 2014, 'historical')\n",
    "aod245 = read_aod(2085, 2099, 'ssp245')\n",
    "aod370 = read_aod(2085, 2099, 'ssp370')\n",
    "aod585 = read_aod(2085, 2099, 'ssp585')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82669932-7214-4a8e-b607-496e2c792784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### merge data sets with aod data Set###\n",
    "cmh = read_pangeo(2000, 2014, 'historical')\n",
    "dh = cmh.merge(aodh.drop('time_bnds'))\n",
    "\n",
    "cm245 = read_pangeo(2085, 2099, 'ssp245')\n",
    "d245= cm245.merge(aod245.drop('time_bnds'))\n",
    "\n",
    "cm585 = read_pangeo(2085, 2099, 'ssp585')\n",
    "d585 = cm585.merge(aod585.drop('time_bnds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8ddf6-697c-4a8d-b8b8-6a0879a34a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm370 = read_pangeo(2085, 2099, 'ssp370')\n",
    "cm370b  = read_370(2085,2099)\n",
    "d370 = cm370.merge(cm370b).merge(aod370.drop('time_bnds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c022e33-0450-46cb-a17b-6550591a8f4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -101] NetCDF: HDF error: b'/home/jovyan/Tjaernoe2022-group5/notebooks/Lea/ssp370_combined_q94.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/file_manager.py:201\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/jovyan/Tjaernoe2022-group5/notebooks/Lea/ssp370_combined_q94.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m arh \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_hist_q94.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m ar245 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssp245_combined_q94.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m ar370 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mssp370_combined_q94.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ar585 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssp585_combined_q94.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/api.py:531\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    520\u001b[0m     decode_cf,\n\u001b[1;32m    521\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    528\u001b[0m )\n\u001b[1;32m    530\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 531\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    538\u001b[0m     backend_ds,\n\u001b[1;32m    539\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:555\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    536\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m ):\n\u001b[1;32m    554\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 555\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:384\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    378\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    379\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    382\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:332\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:387\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    388\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/file_manager.py:189\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/file_manager.py:207\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    205\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    206\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 207\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2353\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:1963\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -101] NetCDF: HDF error: b'/home/jovyan/Tjaernoe2022-group5/notebooks/Lea/ssp370_combined_q94.nc'"
     ]
    }
   ],
   "source": [
    "arh = xr.open_dataset('combined_hist_q94.nc')\n",
    "ar245 = xr.open_dataset('ssp245_combined_q94.nc')\n",
    "ar370 = xr.open_dataset('ssp370_combined_q94.nc')\n",
    "ar585 = xr.open_dataset('ssp585_combined_q94.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a92b7-95bb-48bc-bd1f-0d0695096e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice to poles and slice only until pressure levels where AR can be detected\n",
    "n245 = d245.sel(lat = slice(60,90),plev=slice(100000, 25000))\n",
    "s245 =d245.sel(lat = slice(-90,-60),plev=slice(100000, 25000))\n",
    "\n",
    "n370 = d370.sel(lat = slice(60,90),plev=slice(100000, 25000))\n",
    "s370 =d370.sel(lat = slice(-90,-60),plev=slice(100000, 25000))\n",
    "\n",
    "n585 = d585.sel(lat = slice(60,90),plev=slice(100000, 25000))\n",
    "s585 =d585.sel(lat = slice(-90,-60),plev=slice(100000, 25000))\n",
    "\n",
    "nh = dh.sel(lat = slice(60,90), plev=slice(100000, 25000))\n",
    "sh =dh.sel(lat = slice(-90,-60), plev=slice(100000, 25000))\n",
    "\n",
    "\n",
    "## mask data for being inside or outside of a atmospheric river\n",
    "masked_n245 = n245.where(ar245['ivt']==True)\n",
    "masked_n370 = n370.where(ar370['ivt']==True)\n",
    "masked_n585 = n585.where(ar585['ivt']==True)\n",
    "masked_nh = nh.where(arh['ivt']==True)\n",
    "neg_n245 = n245.where(ar245['ivt']==False | n245['va'] > 0 )\n",
    "neg_n370 = n370.where(ar370['ivt']==False | n370['va'] > 0)\n",
    "neg_n585 = n585.where(ar585['ivt']==False | n585['va'] > 0)\n",
    "neg_nh = nh.where(arh['ivt']==False | nh['va'] > 0)\n",
    "\n",
    "# antarctic\n",
    "masked_s245 = s245.where(ar245['ivt']==True)\n",
    "masked_s370 = s370.where(ar370['ivt']==True)\n",
    "masked_s585 = s585.where(ar585['ivt']==True)\n",
    "masked_sh = sh.where(arh['ivt']==True)\n",
    "neg_s245 = s245.where(ar245['ivt']==False | s245['va'] > 0)\n",
    "neg_s370 = s370.where(ar370['ivt']==False | s370['va'] > 0)\n",
    "neg_s585 = s585.where(ar585['ivt']==False | s585['va'] > 0)\n",
    "neg_sh = sh.where(arh['ivt']==False | sh['va'] > 0)\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988b275-c268-47c0-89ef-0e0ffb4ccb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treat variables for plotting\n",
    "#integrate needed humidity\n",
    "int_nh245 =-1*masked_n245['hus'].integrate('plev')\n",
    "int_sh245 =-1*masked_s245['hus'].integrate('plev')\n",
    "int_nh370 =-1*masked_n370['hus'].integrate('plev')\n",
    "int_sh370 =-1*masked_s370['hus'].integrate('plev')\n",
    "int_nh585 =-1*masked_n585['hus'].integrate('plev')\n",
    "int_sh585 =-1*masked_s585['hus'].integrate('plev')\n",
    "int_nhh =-1*masked_nh['hus'].integrate('plev')\n",
    "int_shh =-1*masked_sh['hus'].integrate('plev')\n",
    "                                                       \n",
    "neg_nih = -1*neg_nh['hus'].integrate('plev')                                                    \n",
    "neg_sih = -1*neg_sh['hus'].integrate('plev')  \n",
    "# flatten and remove na\n",
    "    # humidity                                                      \n",
    "int_nh245 = int_nh245.values.flatten() # for plotting with matplotlib: flatten db to array \n",
    "int_sh245 = int_sh245.values.flatten()\n",
    "int_nh370 = int_nh370.values.flatten()\n",
    "int_sh370 = int_sh370.values.flatten()\n",
    "int_nh585 = int_nh585.values.flatten()\n",
    "int_sh585 = int_sh585.values.flatten()\n",
    "int_nhh = int_nhh.values.flatten()\n",
    "int_shh = int_shh.values.flatten()\n",
    "neg_nih = neg_nh.values.flatten()\n",
    "neg_sih = neg_sh.values.flatten()                                                      \n",
    "int_nh245 = int_nh245[~np.isnan(int_nh245)] # remove na from dataset to be able to weight distribution\n",
    "int_sh245 = int_sh245[~np.isnan(int_sh245)]\n",
    "int_nh370 = int_nh370[~np.isnan(int_nh370)]\n",
    "int_sh370 = int_sh370[~np.isnan(int_sh370)]\n",
    "int_nh585 = int_nh585[~np.isnan(int_nh585)]\n",
    "int_sh585 = int_sh585[~np.isnan(int_sh585)]\n",
    "int_nhh = int_nhh[~np.isnan(int_nhh)]\n",
    "int_shh = int_shh[~np.isnan(int_shh)] \n",
    "neg_nih = neg_nih[~np.isnan(neg_nih)]\n",
    "neg_sih = neg_sih[~np.isnan(neg_sih)]                                                                                                     \n",
    "int_hum = pd.DataFrame(data=[int_nh245,int_sh245,int_nh370,int_sh370,int_nh585,int_sh585,int_nhh,int_shh,neg_nih,neg_sih]).T\n",
    "int_hum.columns=['int_nh245','int_sh245','int_nh370','int_sh370','int_nh585','int_sh585','int_nhh','int_shh','neg_nih','neg_sih']                                                       \n",
    "    # AOD\n",
    "na245 = masked_n245['od550aer'].values.flatten()  # for plotting with matplotlib: flatten db to array and substract average\n",
    "sa245 = masked_s245['od550aer'].values.flatten()\n",
    "na370 = masked_n370['od550aer'].values.flatten()\n",
    "sa370 = masked_s370['od550aer'].values.flatten()\n",
    "na585 = masked_n585['od550aer'].values.flatten()\n",
    "sa585 = masked_s585['od550aer'].values.flatten()\n",
    "nah = masked_nh['od550aer'].values.flatten()\n",
    "sah = masked_sh['od550aer'].values.flatten()\n",
    "neg_nah = neg_nah['od550aer'].values.flatten() \n",
    "neg_sah = neg_sah['od550aer'].values.flatten()                                                      \n",
    "na245 = na245[~np.isnan(na245)] # remove na from dataset to be able to weight distribution\n",
    "sa245 = sa245[~np.isnan(sa245)]\n",
    "na370 = na370[~np.isnan(na370)]\n",
    "sa370 = sa370[~np.isnan(sa370)]\n",
    "na585 = na585[~np.isnan(na585)]\n",
    "sa585 = sa585[~np.isnan(sa585)]\n",
    "nah = nah[~np.isnan(nah)]\n",
    "sah = sah[~np.isnan(sah)]\n",
    "neg_nah = neg_nah[~np.isnan(neg_nah)]\n",
    "neg_sah = neg_sah[~np.isnan(neg_sah)]                                                        \n",
    "aod = pd.DataFrame(data=[na245,sa245,na370,sa370,na585,sa585,nah,sah,neg_nah,neg_sah]).T \n",
    "aod.columns = ['na245','sa245','na370','sa370','na585','sa585','nah','sah','neg_nah','neg_sah']   \n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea886ac-8088-45d1-a93d-1086a3389fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # cloud cover\n",
    "nc245 = masked_n245['clt'].values.flatten()  # for plotting with matplotlib: flatten db to array and substract average\n",
    "sc245 = masked_s245['clt'].values.flatten()\n",
    "nc370 = masked_n370['clt'].values.flatten()\n",
    "sc370 = masked_s370['clt'].values.flatten()\n",
    "nc585 = masked_n585['clt'].values.flatten()\n",
    "sc585 = masked_s585['clt'].values.flatten()\n",
    "nch = masked_nh['clt'].values.flatten()\n",
    "sch = masked_sh['clt'].values.flatten()\n",
    "neg_nch = neg_nh['clt'].values.flatten() \n",
    "neg_sch = neg_sh['clt'].values.flatten()                                                       \n",
    "nc245 = na245[~np.isnan(nc245)] # remove na from dataset to be able to weight distribution\n",
    "sc245 = sa245[~np.isnan(sc245)]\n",
    "nc370 = na370[~np.isnan(nc370)]\n",
    "sc370 = sa370[~np.isnan(sc370)]\n",
    "nc585 = na585[~np.isnan(nc585)]\n",
    "sc585 = sa585[~np.isnan(sc585)]\n",
    "nch = nah[~np.isnan(nch)]\n",
    "sch = sah[~np.isnan(sch)]\n",
    "neg_nch = neg_nch[~np.isnan(neg_nch)]\n",
    "neg_sch = neg_sch[~np.isnan(neg_sch)]                                                        \n",
    "cloud = pd.DataFrame(data=[nc245,sc245,nc370,sc370,nc585,sc585,nch,schn, neg_nch, neg_sch]).T \n",
    "cloud.columns = ['nc245','sc245','nc370','sc370','nc585','sc585','nch','sch','neg_nch', 'neg_sch']   \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10f0c1-0ed5-4898-a837-2c2f75ee5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # precipitation                                                      \n",
    "np245 = masked_n245['pr'].values.flatten()  # for plotting with matplotlib: flatten db to array and substract average\n",
    "sp245 = masked_s245['pr'].values.flatten()\n",
    "np370 = masked_n370['pr'].values.flatten()\n",
    "sp370 = masked_s370['pr'].values.flatten()\n",
    "np585 = masked_n585['pr'].values.flatten()\n",
    "sp585 = masked_s585['pr'].values.flatten()\n",
    "nph = masked_nh['pr'].values.flatten()\n",
    "sph = masked_sh['pr'].values.flatten()\n",
    "neg_nph = neg_nh['pr'].values.flatten() \n",
    "neg_sph = neg_sh['pr'].values.flatten()                                                         \n",
    "np245 = np245[~np.isnan(np245)] # remove na from dataset to be able to weight distribution\n",
    "sp245 = sp245[~np.isnan(sp245)]\n",
    "np370 = np370[~np.isnan(np370)]\n",
    "sp370 = sp370[~np.isnan(sp370)]\n",
    "np585 = np585[~np.isnan(np585)]\n",
    "sp585 = sp585[~np.isnan(sp585)]\n",
    "nph = nph[~np.isnan(nph)]\n",
    "sph = sph[~np.isnan(sph)]\n",
    "neg_nph = neg_nph[~np.isnan(neg_nph)]\n",
    "neg_sph = neg_sph[~np.isnan(neg_sph)]                                                       \n",
    "np245 =np245[np245 >0.0000024099] # exclude weird small values\n",
    "sp245=sp245[sp245>0.0000024099]\n",
    "np370=np370[np370>0.0000024099]\n",
    "sp370=sp370[sp370>0.0000024099]\n",
    "np585=np585[np585>0.0000024099]\n",
    "sp585=sp585[sp585>0.0000024099]\n",
    "nph =nph[nph >0.0000024099]\n",
    "sph=sph[sph>0.0000024099]\n",
    "neg_nph = neg_nph[neg_nph>0.0000024099]\n",
    "neg_sph = neg_sph[neg_sph>0.0000024099]                                                        \n",
    "np245= np245*60*60*24 # from precipitation \"per second\" to \"per day\"\n",
    "sp245= sp245*60*60*24\n",
    "np370= np370*60*60*24\n",
    "sp370= sp370*60*60*24\n",
    "np585= np585*60*60*24\n",
    "sp585= sp585 *60*60*24\n",
    "nph= nph*60*60*24\n",
    "sph= sph*60*60*24\n",
    "neg_nph= neg_nph*60*60*24\n",
    "neg_sph= neg_sph*60*60*24                                                       \n",
    "precip = pd.DataFrame(data=[np245,sp245,np370,sp370,np585,sp585,nph,sph,neg_nph,neg_sph]).T \n",
    "precip.columns = ['np245','sp245','np370','sp370','np585','sp585','nph','sph','neg_nph','neg_sph']               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169397de-0721-479a-937e-1f562104fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                       \n",
    "    # surface temperature\n",
    "#create average temperature for research area during modelling period to calculate anomaly\n",
    "avtn245 = n245['tas'].mean(['time','lat','lon'])\n",
    "avts245 = s245['tas'].mean(['time','lat','lon'])\n",
    "avtn370 = n370['tas'].mean(['time','lat','lon'])\n",
    "avts370 = s370['tas'].mean(['time','lat','lon'])\n",
    "avtn585 = n585['tas'].mean(['time','lat','lon'])\n",
    "avts585 = s585['tas'].mean(['time','lat','lon'])\n",
    "avtnh =nh['tas'].mean(['time','lat','lon'])\n",
    "avtsh = sh['tas'].mean(['time','lat','lon'])\n",
    "nt245 = (n245['tas']-avtn245).values.flatten() # for plotting with matplotlib: flatten db to array and substract average\n",
    "st245 = (s245['tas']-avts245).values.flatten()\n",
    "nt370 = (n370['tas']-avtn370).values.flatten()\n",
    "st370 = (s370['tas']-avts370).values.flatten()\n",
    "nt585 = (n585['tas']-avtn585).values.flatten()\n",
    "st585 = (s585['tas']-avts585).values.flatten()\n",
    "nth = (nh['tas']-avtnh).values.flatten()\n",
    "sth = (sh['tas']-avtsh).values.flatten()\n",
    "nt245 = nt245[~np.isnan(nt245)] # remove na from dataset to be able to weight distribution\n",
    "st245 = st245[~np.isnan(st245)]\n",
    "nt370 = nt370[~np.isnan(nt370)]\n",
    "st370 = st370[~np.isnan(st370)]\n",
    "nt585 = nt585[~np.isnan(nt585)]\n",
    "st585 = st585[~np.isnan(st585)]\n",
    "nth = nth[~np.isnan(nth)]\n",
    "tph = sth[~np.isnan(sth)]\n",
    "temp_anom = pd.DataFrame(data=[nt245,st245,nt370,st370,nt585,st585,nth,sth]).T \n",
    "temp_anom.columns = ['nt245','st245','nt370','st370','nt585','st585','nth','sth']   \n",
    "#for comparison inside and outside ARs \n",
    "nthin = nh['tas'].values.flatten()\n",
    "nthin = nthin[~np.isnan(nthin)] \n",
    "sthin = sh['tas'].values.flatten()\n",
    "sthin = sthin[~np.isnan(sthin)] \n",
    "neg_nth = neg_nh['tas'].values.flatten() \n",
    "neg_sth = neg_sh['tas'].values.flatten() \n",
    "neg_nth = neg_nth[~np.isnan(neg_nth)]\n",
    "neg_sth = neg_sth[~np.isnan(neg_sth)]\n",
    "temp = pd.DataFrame(data = ['nthin', 'sthin', 'neg_nth', 'neg_sth'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8818b-d175-4f14-84cc-2199f0e8bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size for wilcoxon test arctic\n",
    "   # humidity\n",
    "nh370sized = int_nh370[np.random.randint(0, len(int_nh370), 10000)]\n",
    "nh245sized = int_nh245[np.random.randint(0, len(int_nh245), 10000)]\n",
    "nh585sized = int_nh585[np.random.randint(0, len(int_nh585), 10000)]\n",
    "nhhsized =int_nhh[np.random.randint(0, len(int_nhh), 10000)]\n",
    "sized_hum = pd.DataFrame(data = [nh370sized, nh245sized , nh585sized, nhhsized]).T\n",
    "sized_hum.columns = ['nh370sized', 'nh245sized' , 'nh585sized', 'nhhsized']    \n",
    "   # AOD \n",
    "na370sized = na370[np.random.randint(0, len(na370), 10000)]\n",
    "na245sized = na245[np.random.randint(0, len(na245), 10000)]\n",
    "na585sized = na585[np.random.randint(0, len(na585), 10000)]\n",
    "nahsized = nah[np.random.randint(0, len(nah), 10000)] \n",
    "sized_aod = pd.DataFrame(data = [na370sized, na245sized , na585sized, nahsized]).T \n",
    "sized_aod.columns = ['na370sized', 'na245sized' , 'na585sized', 'nahsized']                                                     \n",
    "   # precipitation \n",
    "np370sized = np370[np.random.randint(0, len(np370), 10000)]\n",
    "np245sized = np245[np.random.randint(0, len(np245), 10000)]\n",
    "np585sized = np585[np.random.randint(0, len(np585), 10000)]\n",
    "nphsized = nph[np.random.randint(0, len(nph), 10000)]\n",
    "sized_precip = pd.DataFrame(data = [np370sized, np245sized , np585sized, nphsized]).T   \n",
    "sized_precip.columns = ['np370sized', 'np245sized' , 'np585sized', 'nphsized']                                                        \n",
    "     # temperature \n",
    "nt370sized = nt370[np.random.randint(0, len(nt370), 10000)]\n",
    "nt245sized = nt245[np.random.randint(0, len(nt245), 10000)]\n",
    "nt585sized = nt585[np.random.randint(0, len(nt585), 10000)]\n",
    "nthsized = nth[np.random.randint(0, len(nth), 10000)]                                                         \n",
    "sized_temp = pd.DataFrame(data = [nt370sized, nt245sized , nt585sized, nthsized]).T     \n",
    "sized_temp.columns = ['nt370sized', 'nt245sized' , 'nt585sized', 'nthsized'] \n",
    "     # cloud cover\n",
    "nc370sized = nc370[np.random.randint(0, len(nc370), 10000)]\n",
    "nc245sized = nc245[np.random.randint(0, len(nc245), 10000)]\n",
    "nc585sized = nc585[np.random.randint(0, len(nc585), 10000)]\n",
    "nchsized = nch[np.random.randint(0, len(nch), 10000)]\n",
    "sized_cloud = pd.DataFrame(data = [nc370sized, nc245sized , nc585sized, nchsized]).T \n",
    "sized_cloud.columns = ['nc370sized', 'nc245sized' , 'nc585sized', 'nchsized']                                                        \n",
    "                                                       \n",
    "# size for wilcoxon test antarctic\n",
    "   # humidity\n",
    "sh370sized = int_sh370[np.random.randint(0, len(int_sh370), 10000)]\n",
    "sh245sized = int_sh245[np.random.randint(0, len(int_sh245), 10000)]\n",
    "sh585sized = int_sh585[np.random.randint(0, len(int_sh585), 10000)]\n",
    "shhsized =int_shh[np.random.randint(0, len(int_shh), 10000)]\n",
    "ssized_hum = pd.DataFrame(data = [sh370sized, sh245sized , sh585sized, shhsized]).T     \n",
    "ssized_hum.columns = ['sh370sized', 'sh245sized' , 'sh585sized', 'shhsized']                                                        \n",
    "   # AOD \n",
    "sa370sized = sa370[np.random.randint(0, len(sa370), 10000)]\n",
    "sa245sized = sa245[np.random.randint(0, len(sa245), 10000)]\n",
    "sa585sized = sa585[np.random.randint(0, len(sa585), 10000)]\n",
    "sahsized = sah[np.random.randint(0, len(sah), 10000)] \n",
    "ssized_aod = pd.DataFrame(data = [sa370sized, sa245sized , sa585sized, sahsized]).T  \n",
    "ssized_aod.columns = ['sa370sized', 'sa245sized' , 'sa585sized', 'sahsized']                                                        \n",
    "   # precipitation \n",
    "sp370sized = sp370[np.random.randint(0, len(sp370), 10000)]\n",
    "sp245sized = sp245[np.random.randint(0, len(sp245), 10000)]\n",
    "sp585sized = sp585[np.random.randint(0, len(sp585), 10000)]\n",
    "sphsized = sph[np.random.randint(0, len(sph), 10000)]\n",
    "ssized_precip = pd.DataFrame(data = [sp370sized, sp245sized , sp585sized, sphsized]).T  \n",
    "ssized_precip.columns = ['sp370sized', 'sp245sized' , 'sp585sized', 'sphsized']                                                        \n",
    "     # temperature \n",
    "st370sized = st370[np.random.randint(0, len(st370), 10000)]\n",
    "st245sized = st245[np.random.randint(0, len(st245), 10000)]\n",
    "st585sized = st585[np.random.randint(0, len(st585), 10000)]\n",
    "sthsized = sth[np.random.randint(0, len(sth), 10000)]  \n",
    "ssized_temp = pd.DataFrame(data = [st370sized, st245sized , st585sized, sthsized]).T                        \n",
    "ssized_temp.columns = ['st370sized', 'st245sized' , 'st585sized', 'sthsized']                                                        \n",
    "     # cloud cover\n",
    "sc370sized = sc370[np.random.randint(0, len(sc370), 10000)]\n",
    "sc245sized = sc245[np.random.randint(0, len(sc245), 10000)]\n",
    "sc585sized = sc585[np.random.randint(0, len(sc585), 10000)]\n",
    "schsized = sch[np.random.randint(0, len(sch), 10000)]\n",
    "ssized_cloud = pd.DataFrame(data = [sc370sized, sc245sized , sc585sized, schsized]).T                      \n",
    "ssized_cloud.columns = ['sc370sized', 'sc245sized' , 'sc585sized', 'schsized']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bfda5-2b78-4c06-933c-ae5d6462704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot figures ####                                                     \n",
    "fig, axs = plt.subplots(2,2, figsize=(10,6))\n",
    "fig.suptitle('Arctic, inside AR', fontsize=14)\n",
    "axs = axs.flatten()\n",
    "axs[0].hist(int_hum['int_nh245'], weights = np.zeros_like(int_hum['int_nh245'])+1./int_hum['int_nh245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins= 200)\n",
    "axs[0].hist(int_hum['int_nh370'], weights = np.zeros_like(int_hum['int_nh370'])+1./int_hum['int_nh370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins= 200)\n",
    "axs[0].hist(int_hum['int_nh585'], weights = np.zeros_like(int_hum['int_nh585'])+1./int_hum['int_nh585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins= 200)\n",
    "axs[0].hist(int_hum['int_nhh'], weights = np.zeros_like(int_hum['int_nhh'])+1./int_hum['int_nhh'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins= 200)\n",
    "axs[0].set( xlabel = 'integrated specific humidity [kg/kg]', ylabel = 'frequency')\n",
    "\n",
    "axs[1].hist(precip['np245'], weights = np.zeros_like(precip['np245'])+1./precip['np245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins=np.logspace(np.log10(np245.min()),np.log10(np245.max()),100))\n",
    "axs[1].hist(precip['np370'], weights = np.zeros_like(precip['np370'])+1./precip['np370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins=np.logspace(np.log10(np370.min()),np.log10(np370.max()),100))\n",
    "axs[1].hist(precip['np585'], weights = np.zeros_like(precip['np585'])+1./precip['np585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins=np.logspace(np.log10(np585.min()),np.log10(np585.max()),100))\n",
    "axs[1].hist(precip['nph'], weights = np.zeros_like(precip['nph'])+1./precip['nph'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins=np.logspace(np.log10(nph.min()),np.log10(nph.max()),100)) \n",
    "axs[1].set(yscale ='log', xlabel = 'Precipitation (mm/d)', ylabel = 'frequency')\n",
    "axs[1].legend() \n",
    "\n",
    "axs[2].hist(aod['na245'], weights = np.zeros_like(aod['na245'])+1./aod['na245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins= 200, )\n",
    "axs[2].hist(aod['na370'], weights = np.zeros_like(aod['na370'])+1./aod['na370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins= 200,)\n",
    "axs[2].hist(aod['na585'], weights = np.zeros_like(aod['na585'])+1./aod['na585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins= 200, )\n",
    "axs[2].hist(aod['nah'], weights = np.zeros_like(aod['nah'])+1./aod['nah'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins= 200, )\n",
    "axs[2].set( xlim = (0,2.5), xlabel = 'AODat 500 nm', ylabel = 'frequency', yscale = 'log')\n",
    "\n",
    "axs[3].hist(temp_anom['nt245'], weights = np.zeros_like(temp_anom['nt245'])+1./temp_anom['nt245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins = 200)\n",
    "axs[3].hist(tem_anom['nt370'], weights = np.zeros_like(tem_anom['nt370'])+1./tem_anom['nt370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins = 200)\n",
    "axs[3].hist(temp_anom['nt585'], weights = np.zeros_like(temp_anom['nt585'])+1./temp_anom['nt585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins = 200)\n",
    "axs[3].hist(temp_anom['nth'], weights = np.zeros_like(temp_anom['nth'])+1./temp_anom['nth'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins = 200)\n",
    "axs[3].set(xlabel = 'Temperature anomaly (K)', ylabel = 'frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Arctic_future.png')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(10,6))\n",
    "fig.suptitle('Antarctic, inside AR', fontsize=14)\n",
    "axs = axs.flatten()\n",
    "axs[0].hist(int_hum['int_sh245'], weights = np.zeros_like(int_hum['int_sh245'])+1./int_hum['int_sh245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins= 200)\n",
    "axs[0].hist(int_hum['int_sh370'], weights = np.zeros_like(int_hum['int_sh370'])+1./int_hum['int_sh370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins= 200)\n",
    "axs[0].hist(int_hum['int_sh585'], weights = np.zeros_like(int_hum['int_sh585'])+1./int_hum['int_sh585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins= 200)\n",
    "axs[0].hist(int_hum['int_shh'], weights = np.zeros_like(int_hum['int_shh'])+1./int_hum['int_shh'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins= 200)\n",
    "axs[0].set( xlabel = 'integrated specific humidity [kg/kg]', ylabel = 'frequency')\n",
    "\n",
    "axs[1].hist(precip['sp245'], weights = np.zeros_like(precip['sp245'])+1./precip['sp245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins=np.logspace(np.log10(np245.min()),np.log10(np245.max()),100))\n",
    "axs[1].hist(precip['sp370'], weights = np.zeros_like(precip['sp370'])+1./precip['sp370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins=np.logspace(np.log10(np370.min()),np.log10(np370.max()),100))\n",
    "axs[1].hist(precip['sp585'], weights = np.zeros_like(precip['sp585'])+1./precip['sp585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins=np.logspace(np.log10(np585.min()),np.log10(np585.max()),100))\n",
    "axs[1].hist(precip['sph'], weights = np.zeros_like(precip['sph'])+1./precip['sph'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins=np.logspace(np.log10(nph.min()),np.log10(nph.max()),100)) \n",
    "axs[1].set(yscale ='log', xlabel = 'Precipitation (mm/d)', ylabel = 'frequency')\n",
    "axs[1].legend() \n",
    "\n",
    "axs[2].hist(sa245, weights = np.zeros_like(sa245)+1./sa245.size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins= 200, )\n",
    "axs[2].hist(sa370, weights = np.zeros_like(sa370)+1./sa370.size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins= 200,)\n",
    "axs[2].hist(sa585, weights = np.zeros_like(sa585)+1./sa585.size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins= 200, )\n",
    "axs[2].hist(sah, weights = np.zeros_like(sah)+1./sah.size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins= 200, )\n",
    "axs[2].set( xlim = (0,2.5), xlabel = 'AODat 500 nm', ylabel = 'frequency', yscale = 'log')\n",
    "\n",
    "axs[2].hist(aod['sa245'], weights = np.zeros_like(aod['sa245'])+1./aod['sa245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins= 200, )\n",
    "axs[2].hist(aod['sa370'], weights = np.zeros_like(aod['sa370'])+1./aod['sa370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins= 200,)\n",
    "axs[2].hist(aod['sa585'], weights = np.zeros_like(aod['sa585'])+1./aod['sa585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins= 200, )\n",
    "axs[2].hist(aod['sah'], weights = np.zeros_like(aod['sah'])+1./aod['sah'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins= 200, )\n",
    "axs[2].set( xlim = (0,2.5), xlabel = 'AODat 500 nm', ylabel = 'frequency', yscale = 'log')\n",
    "\n",
    "axs[3].hist(temp_anom['st245'], weights = np.zeros_like(temp_anom['st245'])+1./temp_anom['st245'].size, \n",
    "        fill = False, edgecolor='g', histtype = 'step', linewidth = 1, label = \"ssp245\", bins = 200)\n",
    "axs[3].hist(tem_anom['st370'], weights = np.zeros_like(tem_anom['st370'])+1./tem_anom['st370'].size, \n",
    "        fill = False, edgecolor='b', histtype = 'step', linewidth = 1, label = \"ssp370\", bins = 200)\n",
    "axs[3].hist(temp_anom['st585'], weights = np.zeros_like(temp_anom['st585'])+1./temp_anom['st585'].size, \n",
    "        fill = False, edgecolor='y',  histtype = 'step', linewidth = 1,label = \"ssp585\", bins = 200)\n",
    "axs[3].hist(temp_anom['sth'], weights = np.zeros_like(temp_anom['sth'])+1./temp_anom['sth'].size, \n",
    "        fill = False, edgecolor='k', histtype = 'step', linewidth = 1, label = \"historical\", bins = 200)\n",
    "axs[3].set(xlabel = 'Temperature anomaly (K)', ylabel = 'frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Antarctic_future.png')\n",
    "                                                       \n",
    "                                                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
