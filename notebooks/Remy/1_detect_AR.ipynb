{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xr.set_options(display_style='html')\n",
    "import intake\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib.colors import LogNorm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.path as mpath\n",
    "from functions import compute_ivt,to_nc\n",
    "from matplotlib import rc,animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(source_id=['NorESM2-LM'], experiment_id=['historical'], table_id=['day'], variable_id=['hus','va'], member_id=['r1i1p1f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = list(dset_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset_dict[dataset_list[0]]\n",
    "dset = dset.sel(member_id='r1i1p1f1',time=slice(str(year)+\"-01-01\", str(year)+\"-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vas = dset.va\n",
    "hus = dset.hus\n",
    "plev = dset.plev\n",
    "lat_ = hus.lat\n",
    "lon_ = hus.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivt = compute_ivt(hus,vas,plev)\n",
    "dset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivt_ns = ivt.copy()\n",
    "ivt_ns = xr.where(ivt_ns.lat<0,-ivt_ns,ivt_ns,True) # minus for southern hemisphere (positive toward the pole)\n",
    "ivt_ns_pos = xr.where(ivt_ns<0,ivt_ns*0,ivt_ns,True) # negative values = not poleward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-computed 98th percentile IVT\n",
    "q98 = xr.open_dataset('q93_2000.nc')\n",
    "q98 = q98.rename({'__xarray_dataarray_variable__':'ivt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "excess = ivt_ns_pos-q98\n",
    "\n",
    "q98.close()\n",
    "ivt_ns_pos.close()\n",
    "\n",
    "ar_points = xr.where(excess>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7330/1875809623.py:15: UserWarning: No contour levels were found within the data range.\n",
      "  ll = plt.contour(df_loc,levels=[0,1])\n"
     ]
    }
   ],
   "source": [
    "out_ar = ar_points.copy()\n",
    "ar_points.close()\n",
    "\n",
    "out_ar = out_ar.drop_vars(['quantile','member_id'])\n",
    "\n",
    "out_loc = np.zeros((out_ar.ivt.shape[0],out_ar.ivt.shape[1],out_ar.ivt.shape[2])).astype(int)\n",
    "\n",
    "res_lon = abs(lon_.values[1]-lon_.values[0])\n",
    "res_lat = abs(np.min(np.diff(lat_.values)))\n",
    "\n",
    "test_val = out_ar.ivt.values[:]\n",
    "\n",
    "for tt in range(len(out_ar.time)):\n",
    "    df_loc = test_val[:,tt,:]\n",
    "    ll = plt.contour(df_loc,levels=[0,1])\n",
    "    plt.close()\n",
    "    for item in ll.collections:\n",
    "        for i in item.get_paths():\n",
    "            v = i.vertices\n",
    "            crit = abs(np.max(v[:, 1])-np.min(v[:, 1]))\n",
    "            if (crit>=20): # AR has to be at least 20 deg lat \n",
    "                xx=(v[:, 0]).astype(int)\n",
    "                yy=(v[:, 1]).astype(int)\n",
    "                for (x,y) in zip(xx,yy):\n",
    "                    out_loc[y,tt,x] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ar.ivt.values = out_loc.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ar.to_netcdf(str(year)+'_crit_p93.nc')\n",
    "out_ar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = xr.open_dataset(str(year)+'_crit_p93.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivt_ = AR.ivt\n",
    "ivt = xr.concat([ivt_,ivt_[:,:,:5]],dim='lon')# add extra points for AR at the edge of the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ar = ivt.copy()\n",
    "out_ar.values[:] = out_ar.values[:]*0.0\n",
    "\n",
    "def floodfill(indata):\n",
    "    outloc = np.copy(indata)\n",
    "    for x in range(indata.shape[0]):\n",
    "        # for each latitude get limit longitudes of shapes\n",
    "        ind_lim = np.where(indata[x,:]==1)\n",
    "        for ii in ind_lim:\n",
    "            if len(ii)>1: # if there is a shape at that longitude\n",
    "                jj=0\n",
    "                while jj<=len(ii)-2:\n",
    "                    if abs(ii[jj]-ii[jj+1])>40: # avoid filling where only 1 point in shape\n",
    "                        jj=jj+1\n",
    "                    else:\n",
    "                        outloc[x,ii[jj]:ii[jj+1]]=1\n",
    "                        jj=jj+1\n",
    "    return outloc\n",
    "\n",
    "\n",
    "for k in range(ivt.shape[0]):\n",
    "    matrix = ivt[k,:,:]\n",
    "    tst = floodfill(matrix)\n",
    "    out_ar[k,:,:] = tst\n",
    "    \n",
    "# add the values of the extra points to get ARs at the end of the world\n",
    "out_ar[:,:,:5] = out_ar[:,:,:5]+out_ar[:,:,144:]\n",
    "out_ar = out_ar[:,:,:144]\n",
    "out_ar = xr.where(out_ar>1,1,out_ar)\n",
    "out_ar = out_ar.astype(bool)\n",
    "\n",
    "lat_ = AR.lat\n",
    "lon_ = AR.lon\n",
    "\n",
    "out_ar.to_netcdf(str(year)+'_AR_detection_p93_ok.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
