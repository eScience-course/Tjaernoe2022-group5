{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xr.set_options(display_style='html')\n",
    "import intake\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib.colors import LogNorm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.path as mpath\n",
    "from functions import compute_ivt,to_nc\n",
    "from matplotlib import rc,animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(source_id=['NorESM2-LM'], experiment_id=['historical'], table_id=['day'], variable_id=['hus','va'], member_id=['r1i1p1f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = list(dset_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset_dict[dataset_list[0]]\n",
    "dset = dset.sel(member_id='r1i1p1f1',time=slice(str(year)+\"-01-01\", str(year)+\"-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vas = dset.va\n",
    "hus = dset.hus\n",
    "plev = dset.plev\n",
    "lat_ = hus.lat\n",
    "lon_ = hus.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivt = compute_ivt(hus,vas,plev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivt_ns = ivt.copy()\n",
    "ivt_ns = xr.where(ivt_ns.lat<0,-ivt_ns,ivt_ns,True) # minus for southern hemisphere (positive toward the pole)\n",
    "ivt_ns_pos = xr.where(ivt_ns<0,ivt_ns*0,ivt_ns,True) # negative values = not poleward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-computed 98th percentile IVT\n",
    "q98 = xr.open_dataset('q98.nc')\n",
    "q98 = q98.rename({'__xarray_dataarray_variable__':'ivt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "excess = ivt_ns_pos-q98\n",
    "ar_points = xr.where(excess>0,1,0,keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 96, time: 365, lon: 144)\n",
      "Coordinates:\n",
      "  * lat        (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0\n",
      "  * lon        (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "    quantile   float64 ...\n",
      "    member_id  <U8 'r1i1p1f1'\n",
      "  * time       (time) object 2014-01-01 12:00:00 ... 2014-12-31 12:00:00\n",
      "Data variables:\n",
      "    ivt        (lat, time, lon) int64 dask.array<chunksize=(96, 66, 144), meta=np.ndarray>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/plot/plot.py:1417: UserWarning: No contour levels were found within the data range.\n",
      "  primitive = ax.contour(x, y, z, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'to_nectdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     tt \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mout_ar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_nectdf\u001b[49m()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/common.py:256\u001b[0m, in \u001b[0;36mAttrAccessMixin.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m source[name]\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'to_nectdf'"
     ]
    }
   ],
   "source": [
    "out_ar = ar_points.copy()\n",
    "out_ar.ivt.values[:] = out_ar.ivt.values[:]*0.0\n",
    "\n",
    "res_lon = abs(lon_.values[1]-lon_.values[0])\n",
    "res_lat = abs(lat_.values[1]-lat_.values[0])\n",
    "\n",
    "def to_nc(indata, lat_test, lon_test, tts, fln):\n",
    "    nc = Dataset(fln, 'w')\n",
    "    lat_dim = nc.createDimension('lat', indata.shape[1])\n",
    "    lon_dim = nc.createDimension('lon', indata.shape[2])\n",
    "    t_dim = nc.createDimension('time', len(tts))\n",
    "    lat_var = nc.createVariable('lat', np.float64, ('lat'))\n",
    "    lat_var[:] = lat_test\n",
    "    lon_var = nc.createVariable('lon', np.float64, ('lon'))\n",
    "    lon_var[:] = lon_test\n",
    "    tnd = nc.createVariable('ivt', np.float64, ('time','lat','lon'))\n",
    "    tnd[:,:,:] = indata\n",
    "    times = nc.createVariable('time', np.float64, ('time'))\n",
    "    times[:] = tts\n",
    "    nc.close()\n",
    "\n",
    "tt=0\n",
    "for t in out_ar.time:\n",
    "    df_loc = ar_points.ivt.sel(time=t)\n",
    "    ll = xr.plot.contour(df_loc,levels=[0,1])\n",
    "    for item in ll.collections:\n",
    "        for i in item.get_paths():\n",
    "            v = i.vertices\n",
    "            crit = abs(np.max(v[:, 1])-np.min(v[:, 1]))\n",
    "            if (crit>=20): # AR has to be at least 20 deg lat \n",
    "                xx=(v[:, 0]/res_lon).astype(int)\n",
    "                yy=((v[:, 1]+90)/res_lat).astype(int)\n",
    "                for (x,y) in zip(xx,yy):\n",
    "                    out_ar.ivt[y,tt,x]\n",
    "    tt = tt+1\n",
    "    plt.close()\n",
    "        \n",
    "out_ar.to_nectdf()\n",
    "#from functions import to_nc\n",
    "#to_nc(out_ar, lat_.values, lon_.values, ar_points.time , str(yy)+'_ivt_crit.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = xr.open_dataset(str(yy)+'_crit.nc')\n",
    "ivt_ = AR.ivt\n",
    "ivt = xr.concat([ivt_,ivt_[:,:,:5]],dim='lon') # add extra points for AR at the edge of the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_detect = np.zeros((ivt.shape[0],ivt.shape[1],ivt.shape[2]))\n",
    "\n",
    "def floodfill(indata):\n",
    "    outloc = np.copy(indata)\n",
    "    for x in range(indata.shape[0]):\n",
    "        ind_lim = np.where(indata[x,:]==1)\n",
    "        for ii in ind_lim:\n",
    "            if len(ii)>1:\n",
    "                jj=0\n",
    "                while jj<=len(ii)-2:\n",
    "                    if abs(ii[jj]-ii[jj+1])>10:\n",
    "                        jj=jj+1\n",
    "                    else:\n",
    "                        outloc[x,ii[jj]:ii[jj+1]]=1\n",
    "                        jj=jj+1\n",
    "    return outloc\n",
    "\n",
    "\n",
    "for k in range(ivt.shape[0]):\n",
    "    print(k)\n",
    "    matrix = ivt[k,:,:]\n",
    "    tst = floodfill(matrix)\n",
    "    out_detect[k,:,:] = tst\n",
    "    \n",
    "# add the values of the extra points to get ARs at the end of the world\n",
    "out_detect[:,:,:5] = out_detect[:,:,:5]+out_detect[:,:,144:]\n",
    "out_detect = out_detect[:,:,:144]\n",
    "out_detect[out_detect>1] = 1\n",
    "\n",
    "\n",
    "lat_ = AR.lat\n",
    "lon_ = AR.lon\n",
    "\n",
    "\n",
    "def to_nc(indata, lat_test, lon_test, tts, fln):\n",
    "    nc = Dataset(fln, 'w')\n",
    "    lat_dim = nc.createDimension('lat', indata.shape[1])\n",
    "    lon_dim = nc.createDimension('lon', indata.shape[2])\n",
    "    t_dim = nc.createDimension('time', indata.shape[0])\n",
    "    lat_var = nc.createVariable('lat', np.float64, ('lat'))\n",
    "    lat_var[:] = lat_test\n",
    "    lon_var = nc.createVariable('lon', np.float64, ('lon'))\n",
    "    lon_var[:] = lon_test\n",
    "    tnd = nc.createVariable('ivt', np.float64, ('time','lat','lon'))\n",
    "    tnd[:,:,:] = indata\n",
    "    times = nc.createVariable('time', np.float64, ('time'))\n",
    "    times[:] = tts\n",
    "    nc.close()\n",
    "\n",
    "to_nc(out_ar, lat_.values, lon_.values, range(AR.ivt.shape[0]) , str(year)+'_AR_detection.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
